{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>insured_sex</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_bind_date_year</th>\n",
       "      <th>policy_bind_date_cosine_month</th>\n",
       "      <th>policy_bind_date_sine_month</th>\n",
       "      <th>policy_bind_date_cosine_day</th>\n",
       "      <th>policy_bind_date_sine_day</th>\n",
       "      <th>incident_date_year</th>\n",
       "      <th>incident_date_cosine_month</th>\n",
       "      <th>incident_date_sine_month</th>\n",
       "      <th>incident_date_cosine_day</th>\n",
       "      <th>incident_date_sine_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>5.510911e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-4.286264e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>...</td>\n",
       "      <td>1990</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_state policy_csl  \\\n",
       "0                 328   48         521585           OH    250/500   \n",
       "1                 228   42         342868           IN    250/500   \n",
       "2                 134   29         687698           OH    100/300   \n",
       "3                 256   41         227811           IL    250/500   \n",
       "4                 228   44         367455           IL   500/1000   \n",
       "\n",
       "   policy_deductable  policy_annual_premium  umbrella_limit  insured_zip  \\\n",
       "0               1000                1406.91               0       466132   \n",
       "1               2000                1197.22         5000000       468176   \n",
       "2               2000                1413.14         5000000       430632   \n",
       "3               2000                1415.74         6000000       608117   \n",
       "4               1000                1583.91         6000000       610706   \n",
       "\n",
       "  insured_sex  ... policy_bind_date_year policy_bind_date_cosine_month  \\\n",
       "0        MALE  ...                  2014                  5.000000e-01   \n",
       "1        MALE  ...                  2006                 -1.000000e+00   \n",
       "2      FEMALE  ...                  2000                 -1.836970e-16   \n",
       "3      FEMALE  ...                  1990                 -8.660254e-01   \n",
       "4        MALE  ...                  2014                 -1.000000e+00   \n",
       "\n",
       "  policy_bind_date_sine_month policy_bind_date_cosine_day  \\\n",
       "0               -8.660254e-01               -8.660254e-01   \n",
       "1                1.224647e-16                5.510911e-16   \n",
       "2               -1.000000e+00               -1.000000e+00   \n",
       "3                5.000000e-01                8.660254e-01   \n",
       "4                1.224647e-16               -1.000000e+00   \n",
       "\n",
       "   policy_bind_date_sine_day  incident_date_year incident_date_cosine_month  \\\n",
       "0               5.000000e-01                2015                   0.866025   \n",
       "1               1.000000e+00                2015                   0.866025   \n",
       "2               1.224647e-16                2015                   0.500000   \n",
       "3               5.000000e-01                2015                   0.866025   \n",
       "4               1.224647e-16                2015                   0.500000   \n",
       "\n",
       "  incident_date_sine_month incident_date_cosine_day incident_date_sine_day  \n",
       "0                 0.500000             8.660254e-01               0.500000  \n",
       "1                 0.500000            -4.286264e-16              -1.000000  \n",
       "2                 0.866025             5.000000e-01              -0.866025  \n",
       "3                 0.500000             5.000000e-01              -0.866025  \n",
       "4                 0.866025            -8.660254e-01               0.500000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('featuure_engineered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 0\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated rows:', df.duplicated().sum())\n",
    "print('Missing values:', df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelling import get_preprocessor\n",
    "preprocessor = get_preprocessor(df, 'fraud_reported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = df.drop(columns=['fraud_reported']) \n",
    "y = df['fraud_reported']\n",
    "\n",
    "#performing LabelEncoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "y.shape = (-1,)\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the features\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Y'], dtype='<U1')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0 represents N: not fraud case`\n",
    "\n",
    "`1 represents Y: fraud case`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud_reported\n",
       "N    0.753\n",
       "Y    0.247\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraud_reported'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noted some class imbalance in the target column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    \"\"\"Class to carry out model fiting and cross validation\n",
    "    Attributes:\n",
    "        model: The model \n",
    "        X_train: training features\n",
    "        y_train: target\n",
    "    \"\"\"\n",
    "    def __init__(self, model, preprocessor, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.preprocessor = preprocessor\n",
    "        self.fitted_base_model = None\n",
    "        self.fitted_smote_model = None\n",
    "\n",
    "    def train_base(self):\n",
    "        \"\"\"Fif with only preprocessing which only includes scaling and ohe\"\"\"\n",
    "        # Fit the model\n",
    "        model_pipe = Pipeline([\n",
    "            ('preprocessing', clone(self.preprocessor)),\n",
    "            ('model', clone(self.model))\n",
    "        ])\n",
    "        score = cross_val_score(model_pipe,\n",
    "                                self.X_train,\n",
    "                                self.y_train,\n",
    "                                scoring='accuracy',\n",
    "                                ).mean() \n",
    "        print('**************************************************************************')\n",
    "        print(\"\\n\"f\"The model(with normal preprocessing) has an accuracy of {score*100:.2f}%\")\n",
    "        self.fitted_base_model = model_pipe\n",
    "        self.fitted_base_model.fit(self.X_train, self.y_train)\n",
    "        return self.fitted_base_model\n",
    "        \n",
    "    def train_with_smote(self):\n",
    "        \"\"\"Includes oversampling with smote to reduce class imbalance\"\"\"\n",
    "        # Perform custom Cross-Validation to include smote\n",
    "        scores = []\n",
    "        # splits for cross validation\n",
    "        skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "        for i, (train_idx, test_idx) in enumerate(skf.split(self.X_train, self.y_train)):\n",
    "            # train and test data for the validation\n",
    "            cv_X_train = self.X_train.iloc[train_idx, :]\n",
    "            cv_X_test = self.X_train.iloc[test_idx, :]\n",
    "            cv_y_train = self.y_train[train_idx]\n",
    "            cv_y_test = self.y_train[test_idx]\n",
    "            \n",
    "            preprocessor_clone = clone(self.preprocessor)\n",
    "            \n",
    "            # preprocessing the features\n",
    "            cv_X_train_preprocessed = preprocessor_clone.fit_transform(cv_X_train)\n",
    "            cv_X_test_preprocessed = preprocessor_clone.transform(cv_X_test)\n",
    "            \n",
    "            \n",
    "            # cloning the model\n",
    "            model_clone = clone(self.model)\n",
    "            \n",
    "            # oversampling with smote to fix class imbalance\n",
    "            X_oversampled, y_oversampled = (\n",
    "                SMOTE(random_state=42).fit_resample(cv_X_train_preprocessed,\n",
    "                                                    cv_y_train))\n",
    "            \n",
    "            # convert to dataframe\n",
    "            X_oversampled = pd.DataFrame(X_oversampled, \n",
    "                                         columns=preprocessor_clone.get_feature_names_out())\n",
    "            \n",
    "            # fit and obtain score for the current fold\n",
    "            model_clone.fit(X_oversampled, y_oversampled)\n",
    "            score = model_clone.score(cv_X_test_preprocessed, cv_y_test)\n",
    "            print(f\"Fold {i+1}: Accuracy = {score * 100:.2f}%\")\n",
    "            scores.append(score)\n",
    "        \n",
    "        # define model to return\n",
    "        \n",
    "        # preprocess on whole train data \n",
    "        cv_X_train_preprocessed = preprocessor.fit_transform(self.X_train)\n",
    "        # oversample whole training data\n",
    "        X_oversampled, y_oversampled = (\n",
    "            SMOTE(random_state=42).fit_resample(cv_X_train_preprocessed,\n",
    "                                                self.y_train))\n",
    "        \n",
    "        print('**********************************************************************')\n",
    "        print(\"\\n\"f\"The model(with SMOTE) has an accuracy of {np.mean(scores)*100:.2f}%\")\n",
    "        \n",
    "        # fit and return the model\n",
    "        self.fitted_smote_model = clone(self.model)\n",
    "        self.fitted_smote_model.fit(X_oversampled, y_oversampled)\n",
    "        return self.fitted_smote_model\n",
    "    \n",
    "    def evaluate_on_test(self, X_test, y_test):\n",
    "        \"\"\"evaluates the model on unseen test data\"\"\"\n",
    "        if self.fitted_base_model:\n",
    "            print('Without SMOTE:')\n",
    "            y_pred = self.fitted_base_model.predict(X_test)\n",
    "            print('\\tAccuracy score:', accuracy_score(y_test, y_pred))\n",
    "            print('\\tRecall Score:', recall_score(y_test, y_pred))\n",
    "            print('\\tPrecision Score:', precision_score(y_test, y_pred))\n",
    "            print('\\tF1 Score:', f1_score(y_test, y_pred))\n",
    "            \n",
    "        if self.fitted_smote_model:\n",
    "            print('\\n With SMOTE:')\n",
    "            X_test_preprocessed = self.preprocessor.transform(X_test)\n",
    "            y_pred = self.fitted_smote_model.predict(X_test_preprocessed)\n",
    "            print('\\tAccuracy score:', accuracy_score(y_test, y_pred))\n",
    "            print('\\tRecall Score:', recall_score(y_test, y_pred))\n",
    "            print('\\tPrecision Score:', precision_score(y_test, y_pred))\n",
    "            print('\\tF1 Score:', f1_score(y_test, y_pred))    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 81.73%\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "model_1 = Modeling(log_reg, preprocessor, X_train, y_train)\n",
    "print('Logistic Regression model')\n",
    "fitted_model_1 = model_1.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.00%\n",
      "Fold 2: Accuracy = 75.20%\n",
      "Fold 3: Accuracy = 84.80%\n",
      "Fold 4: Accuracy = 83.20%\n",
      "Fold 5: Accuracy = 84.00%\n",
      "Fold 6: Accuracy = 81.60%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 81.47%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_1_smote = model_1.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "model_2 = Modeling(rf_clf, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 76.40%\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest model')\n",
    "fitted_model_2 = model_2.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 75.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 75.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 77.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 82.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 77.60%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 78.00%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_2_smote = model_2.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    scale_pos_weight=3,\n",
    "    eval_metric='logloss',  # Optional, based on your metric of choice\n",
    "    #use_label_encoder=False\n",
    ")\n",
    "model_3 = Modeling(xgb, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost classifier model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 83.73%\n"
     ]
    }
   ],
   "source": [
    "print('XGBoost classifier model')\n",
    "fitted_model_3 = model_3.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.80%\n",
      "Fold 2: Accuracy = 83.20%\n",
      "Fold 3: Accuracy = 87.20%\n",
      "Fold 4: Accuracy = 87.20%\n",
      "Fold 5: Accuracy = 84.80%\n",
      "Fold 6: Accuracy = 87.20%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 85.07%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_3_smote = model_3.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitted_model_3_smote = model_3.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "model_4 = Modeling(ada, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 80.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('AdaBoost model')\n",
    "fitted_model_4 = model_4.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 83.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 76.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 80.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 81.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 80.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 84.00%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 81.20%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_4_smote = model_4.train_with_smote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model and the xgboost are the best performing with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on unseen test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Logistic Regression******************\n",
      "Without SMOTE:\n",
      "\tAccuracy score: 0.716\n",
      "\tRecall Score: 0.3283582089552239\n",
      "\tPrecision Score: 0.4583333333333333\n",
      "\tF1 Score: 0.3826086956521739\n",
      "\n",
      " With SMOTE:\n",
      "\tAccuracy score: 0.748\n",
      "\tRecall Score: 0.6716417910447762\n",
      "\tPrecision Score: 0.5232558139534884\n",
      "\tF1 Score: 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "print('*********Logistic Regression******************')\n",
    "model_1.evaluate_on_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********XGBoost Classifier*****************\n",
      "Without SMOTE:\n",
      "\tAccuracy score: 0.804\n",
      "\tRecall Score: 0.6268656716417911\n",
      "\tPrecision Score: 0.6363636363636364\n",
      "\tF1 Score: 0.631578947368421\n",
      "\n",
      " With SMOTE:\n",
      "\tAccuracy score: 0.804\n",
      "\tRecall Score: 0.6567164179104478\n",
      "\tPrecision Score: 0.6285714285714286\n",
      "\tF1 Score: 0.6423357664233577\n"
     ]
    }
   ],
   "source": [
    "print('*********XGBoost Classifier*****************')\n",
    "model_3.evaluate_on_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modelling process and evaluation on unseen test data, the XGBoost classifier has higher accuracy but we want a sensitive model and therefore we choose the `Logistic regression model `as the final model due the the high sesnitivity(recall = 67.16%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
