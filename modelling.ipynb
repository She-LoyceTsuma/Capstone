{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>insured_sex</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_bind_date_year</th>\n",
       "      <th>policy_bind_date_cosine_month</th>\n",
       "      <th>policy_bind_date_sine_month</th>\n",
       "      <th>policy_bind_date_cosine_day</th>\n",
       "      <th>policy_bind_date_sine_day</th>\n",
       "      <th>incident_date_year</th>\n",
       "      <th>incident_date_cosine_month</th>\n",
       "      <th>incident_date_sine_month</th>\n",
       "      <th>incident_date_cosine_day</th>\n",
       "      <th>incident_date_sine_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>5.510911e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-4.286264e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>...</td>\n",
       "      <td>1990</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>MALE</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_state policy_csl  \\\n",
       "0                 328   48         521585           OH    250/500   \n",
       "1                 228   42         342868           IN    250/500   \n",
       "2                 134   29         687698           OH    100/300   \n",
       "3                 256   41         227811           IL    250/500   \n",
       "4                 228   44         367455           IL   500/1000   \n",
       "\n",
       "   policy_deductable  policy_annual_premium  umbrella_limit  insured_zip  \\\n",
       "0               1000                1406.91               0       466132   \n",
       "1               2000                1197.22         5000000       468176   \n",
       "2               2000                1413.14         5000000       430632   \n",
       "3               2000                1415.74         6000000       608117   \n",
       "4               1000                1583.91         6000000       610706   \n",
       "\n",
       "  insured_sex  ... policy_bind_date_year policy_bind_date_cosine_month  \\\n",
       "0        MALE  ...                  2014                  5.000000e-01   \n",
       "1        MALE  ...                  2006                 -1.000000e+00   \n",
       "2      FEMALE  ...                  2000                 -1.836970e-16   \n",
       "3      FEMALE  ...                  1990                 -8.660254e-01   \n",
       "4        MALE  ...                  2014                 -1.000000e+00   \n",
       "\n",
       "  policy_bind_date_sine_month policy_bind_date_cosine_day  \\\n",
       "0               -8.660254e-01               -8.660254e-01   \n",
       "1                1.224647e-16                5.510911e-16   \n",
       "2               -1.000000e+00               -1.000000e+00   \n",
       "3                5.000000e-01                8.660254e-01   \n",
       "4                1.224647e-16               -1.000000e+00   \n",
       "\n",
       "   policy_bind_date_sine_day  incident_date_year incident_date_cosine_month  \\\n",
       "0               5.000000e-01                2015                   0.866025   \n",
       "1               1.000000e+00                2015                   0.866025   \n",
       "2               1.224647e-16                2015                   0.500000   \n",
       "3               5.000000e-01                2015                   0.866025   \n",
       "4               1.224647e-16                2015                   0.500000   \n",
       "\n",
       "  incident_date_sine_month incident_date_cosine_day incident_date_sine_day  \n",
       "0                 0.500000             8.660254e-01               0.500000  \n",
       "1                 0.500000            -4.286264e-16              -1.000000  \n",
       "2                 0.866025             5.000000e-01              -0.866025  \n",
       "3                 0.500000             5.000000e-01              -0.866025  \n",
       "4                 0.866025            -8.660254e-01               0.500000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('featuure_engineered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 0\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated rows:', df.duplicated().sum())\n",
    "print('Missing values:', df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelling import get_preprocessor\n",
    "preprocessor = get_preprocessor(df, 'fraud_reported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = df.drop(columns=['fraud_reported']) \n",
    "y = df['fraud_reported']\n",
    "\n",
    "#performing LabelEncoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "y.shape = (-1,)\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the features\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Y'], dtype='<U1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0 represents N: not fraud case`\n",
    "\n",
    "`1 represents Y: fraud case`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud_reported\n",
       "N    0.753\n",
       "Y    0.247\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraud_reported'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noted some class imbalance in the target column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    \"\"\"Class to carry out model fiting and cross validation\n",
    "    Attributes:\n",
    "        model: The model \n",
    "        X_train: training features\n",
    "        y_train: target\n",
    "    \"\"\"\n",
    "    def __init__(self, model, preprocessor, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.preprocessor = preprocessor\n",
    "        self.fitted_base_model = None\n",
    "        self.fitted_smote_model = None\n",
    "\n",
    "    def train_base(self):\n",
    "        \"\"\"Fif with only preprocessing which only includes scaling and ohe\"\"\"\n",
    "        # Fit the model\n",
    "        model_pipe = Pipeline([\n",
    "            ('preprocessing', clone(self.preprocessor)),\n",
    "            ('model', clone(self.model))\n",
    "        ])\n",
    "        score = cross_val_score(model_pipe,\n",
    "                                self.X_train,\n",
    "                                self.y_train,\n",
    "                                scoring='accuracy',\n",
    "                                ).mean() \n",
    "        print('**************************************************************************')\n",
    "        print(\"\\n\"f\"The model(with normal preprocessing) has an accuracy of {score*100:.2f}%\")\n",
    "        self.fitted_base_model = model_pipe\n",
    "        self.fitted_base_model.fit(self.X_train, self.y_train)\n",
    "        return self.fitted_base_model\n",
    "        \n",
    "    def train_with_smote(self):\n",
    "        \"\"\"Includes oversampling with smote to reduce class imbalance\"\"\"\n",
    "        # Perform custom Cross-Validation to include smote\n",
    "        scores = []\n",
    "        # splits for cross validation\n",
    "        skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "        for i, (train_idx, test_idx) in enumerate(skf.split(self.X_train, self.y_train)):\n",
    "            # train and test data for the validation\n",
    "            cv_X_train = self.X_train.iloc[train_idx, :]\n",
    "            cv_X_test = self.X_train.iloc[test_idx, :]\n",
    "            cv_y_train = self.y_train[train_idx]\n",
    "            cv_y_test = self.y_train[test_idx]\n",
    "            \n",
    "            preprocessor_clone = clone(self.preprocessor)\n",
    "            \n",
    "            # preprocessing the features\n",
    "            cv_X_train_preprocessed = preprocessor_clone.fit_transform(cv_X_train)\n",
    "            cv_X_test_preprocessed = preprocessor_clone.transform(cv_X_test)\n",
    "            \n",
    "            \n",
    "            # cloning the model\n",
    "            model_clone = clone(self.model)\n",
    "            \n",
    "            # oversampling with smote to fix class imbalance\n",
    "            X_oversampled, y_oversampled = (\n",
    "                SMOTE(random_state=42).fit_resample(cv_X_train_preprocessed,\n",
    "                                                    cv_y_train))\n",
    "            \n",
    "            # convert to dataframe\n",
    "            X_oversampled = pd.DataFrame(X_oversampled, \n",
    "                                         columns=preprocessor_clone.get_feature_names_out())\n",
    "            \n",
    "            # fit and obtain score for the current fold\n",
    "            model_clone.fit(X_oversampled, y_oversampled)\n",
    "            score = model_clone.score(cv_X_test_preprocessed, cv_y_test)\n",
    "            print(f\"Fold {i+1}: Accuracy = {score * 100:.2f}%\")\n",
    "            scores.append(score)\n",
    "        \n",
    "        # define model to return\n",
    "        \n",
    "        # preprocess on whole train data \n",
    "        cv_X_train_preprocessed = preprocessor.fit_transform(self.X_train)\n",
    "        # oversample whole training data\n",
    "        X_oversampled, y_oversampled = (\n",
    "            SMOTE(random_state=42).fit_resample(cv_X_train_preprocessed,\n",
    "                                                self.y_train))\n",
    "        \n",
    "        print('**********************************************************************')\n",
    "        print(\"\\n\"f\"The model(with SMOTE) has an accuracy of {np.mean(scores)*100:.2f}%\")\n",
    "        \n",
    "        # fit and return the model\n",
    "        self.fitted_smote_model = clone(self.model)\n",
    "        self.fitted_smote_model.fit(X_oversampled, y_oversampled)\n",
    "        return self.fitted_smote_model\n",
    "    \n",
    "    def evaluate_on_test(self, X_test, y_test):\n",
    "        \"\"\"evaluates the model on unseen test data\"\"\"\n",
    "        if self.fitted_base_model:\n",
    "            print('Without SMOTE:')\n",
    "            y_pred = self.fitted_base_model.predict(X_test)\n",
    "            print('\\tAccuracy score:', accuracy_score(y_test, y_pred))\n",
    "            print('\\tRecall Score:', recall_score(y_test, y_pred))\n",
    "            print('\\tPrecision Score:', precision_score(y_test, y_pred))\n",
    "            print('\\tF1 Score:', f1_score(y_test, y_pred))\n",
    "            \n",
    "        if self.fitted_smote_model:\n",
    "            print('\\n With SMOTE:')\n",
    "            X_test_preprocessed = self.preprocessor.transform(X_test)\n",
    "            y_pred = self.fitted_smote_model.predict(X_test_preprocessed)\n",
    "            print('\\tAccuracy score:', accuracy_score(y_test, y_pred))\n",
    "            print('\\tRecall Score:', recall_score(y_test, y_pred))\n",
    "            print('\\tPrecision Score:', precision_score(y_test, y_pred))\n",
    "            print('\\tF1 Score:', f1_score(y_test, y_pred))    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 81.73%\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "model_1 = Modeling(log_reg, preprocessor, X_train, y_train)\n",
    "print('Logistic Regression model')\n",
    "fitted_model_1 = model_1.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 75.20%\n",
      "Fold 3: Accuracy = 84.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 83.20%\n",
      "Fold 5: Accuracy = 84.00%\n",
      "Fold 6: Accuracy = 81.60%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 81.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.00%\n",
      "Fold 2: Accuracy = 75.20%\n",
      "Fold 3: Accuracy = 84.80%\n",
      "Fold 4: Accuracy = 83.20%\n",
      "Fold 5: Accuracy = 84.00%\n",
      "Fold 6: Accuracy = 81.60%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 81.47%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_1_smote = model_1.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "model_2 = Modeling(rf_clf, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 76.80%\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest model')\n",
    "fitted_model_2 = model_2.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 78.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 77.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 76.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 79.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 82.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 77.60%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 78.40%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_2_smote = model_2.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    scale_pos_weight=3,\n",
    "    eval_metric='logloss',  # Optional, based on your metric of choice\n",
    "    #use_label_encoder=False\n",
    ")\n",
    "model_3 = Modeling(xgb, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost classifier model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 83.73%\n"
     ]
    }
   ],
   "source": [
    "print('XGBoost classifier model')\n",
    "fitted_model_3 = model_3.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.80%\n",
      "Fold 2: Accuracy = 83.20%\n",
      "Fold 3: Accuracy = 87.20%\n",
      "Fold 4: Accuracy = 87.20%\n",
      "Fold 5: Accuracy = 84.80%\n",
      "Fold 6: Accuracy = 87.20%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 85.07%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_3_smote = model_3.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 80.80%\n",
      "Fold 2: Accuracy = 83.20%\n",
      "Fold 3: Accuracy = 87.20%\n",
      "Fold 4: Accuracy = 87.20%\n",
      "Fold 5: Accuracy = 84.80%\n",
      "Fold 6: Accuracy = 87.20%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 85.07%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_3_smote = model_3.train_with_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "model_4 = Modeling(ada, preprocessor, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost model\n",
      "**************************************************************************\n",
      "\n",
      "The model(with normal preprocessing) has an accuracy of 80.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mutis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('AdaBoost model')\n",
    "fitted_model_4 = model_4.train_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 83.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Accuracy = 76.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 80.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Accuracy = 81.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 80.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6: Accuracy = 84.00%\n",
      "**********************************************************************\n",
      "\n",
      "The model(with SMOTE) has an accuracy of 81.20%\n"
     ]
    }
   ],
   "source": [
    "fitted_model_4_smote = model_4.train_with_smote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model and the xgboost are the best performing with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on unseen test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Logistic Regression******************\n",
      "Without SMOTE:\n",
      "\tAccuracy score: 0.716\n",
      "\tRecall Score: 0.3283582089552239\n",
      "\tPrecision Score: 0.4583333333333333\n",
      "\tF1 Score: 0.3826086956521739\n",
      "\n",
      " With SMOTE:\n",
      "\tAccuracy score: 0.748\n",
      "\tRecall Score: 0.6716417910447762\n",
      "\tPrecision Score: 0.5232558139534884\n",
      "\tF1 Score: 0.5882352941176472\n"
     ]
    }
   ],
   "source": [
    "print('*********Logistic Regression******************')\n",
    "model_1.evaluate_on_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********XGBoost Classifier*****************\n",
      "Without SMOTE:\n",
      "\tAccuracy score: 0.804\n",
      "\tRecall Score: 0.6268656716417911\n",
      "\tPrecision Score: 0.6363636363636364\n",
      "\tF1 Score: 0.6315789473684211\n",
      "\n",
      " With SMOTE:\n",
      "\tAccuracy score: 0.804\n",
      "\tRecall Score: 0.6567164179104478\n",
      "\tPrecision Score: 0.6285714285714286\n",
      "\tF1 Score: 0.6423357664233577\n"
     ]
    }
   ],
   "source": [
    "print('*********XGBoost Classifier*****************')\n",
    "model_3.evaluate_on_test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modelling process and evaluation on unseen test data, the XGBoost classifier has higher accuracy but we want a sensitive model and therefore we choose the `Logistic regression model `as the final model due the the high sesnitivity(recall = 67.16%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters found:  {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best cross-validation recall score: 0.90\n"
     ]
    }
   ],
   "source": [
    "#Gridsearch\n",
    "log_reg =LogisticRegression()\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10,100,1000],          # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['saga', 'liblinear']  # Solvers compatible with chosen penalties\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "X_train_oversampled ,y_train_oversampled =SMOTE(random_state=42).fit_resample(X_train_preprocessed,y_train)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train_oversampled , y_train_oversampled)\n",
    "\n",
    "# Get best parameters and recall score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation recall score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=1 ,penalty ='l1',solver='liblinear')\n",
    "final_model.fit(X_train_oversampled,y_train_oversampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Unseen Data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7313432835820896"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = final_model.predict(X_test_preprocessed)\n",
    "recall_score(y_test,y_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
